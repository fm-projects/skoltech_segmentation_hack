{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy\n",
    "!pip install torch==1.13.1\n",
    "# Install PyTorch\n",
    "# !conda install pytorch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0 cudatoolkit=11.3 -c pytorch\n",
    "# Install mim\n",
    "!pip install -U openmim\n",
    "# Install mmengine\n",
    "!mim install mmengine\n",
    "# Install MMCV\n",
    "# !pip wheel mmcv-full==1.4.0\n",
    "!mim install mmcv\n",
    "\n",
    "!rm -rf mmsegmentation\n",
    "!git clone -b main https://github.com/open-mmlab/mmsegmentation.git \n",
    "%cd mmsegmentation\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mim download mmsegmentation --config pspnet_r50-d8_4xb2-40k_cityscapes-512x1024 --dest .\n",
    "!mim download mmsegmentation --config configs/ccnet/ccnet_r50-d8_4xb4-80k_ade20k-512x512.py --dest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.metrics import f1_score\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mmseg.apis import init_model, inference_model, show_result_pyplot\n",
    "from mmengine import Config\n",
    "import mmcv\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_VALIDATION = '/kaggle/input/leaders-of-digital-segmentation/patches_val/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate f1-metrics\n",
    "\n",
    "def calculate_model_metrics(model):\n",
    "    real_masks = []\n",
    "    pred_masks = []\n",
    "    for img_path in tqdm(sorted(glob(os.path.join(PATH_TO_VALIDATION, 'images/*')))):\n",
    "        mask_path = img_path.replace('images', 'masks')\n",
    "\n",
    "        mask = cv2.imread(mask_path)\n",
    "        pred_masks += model(img_path).flatten().cpu().detach().numpy().tolist()\n",
    "        real_masks += mask[:, :, 0].flatten().cpu().detach().numpy().tolist()\n",
    "    return f1_score(real_masks, pred_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mmseg.apis import init_model, inference_model, show_result_pyplot\n",
    "from mmengine import Config\n",
    "import mmcv\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "UNET_CHECKPOINT_PATH = '/kaggle/input/checkpoints-sat-snimkis/checkpoint_epoch3_4800.pth'\n",
    "MMCV_CHECKPOINT_PATH = '/kaggle/input/checkpoints-sat-snimkis/mmseq_pspnet_2000.pth'\n",
    "CCNET_CHECKPOINT_PATH = '/kaggle/input/checkpoints-sat-snimkis/ccnet_2000.pth'\n",
    "\n",
    "MMCV_CONF_PATH = 'configs/pspnet/pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py'\n",
    "CCNET_CONF_PATH = 'configs/ccnet/ccnet_r50-d8_4xb4-80k_ade20k-512x512.py'\n",
    "\n",
    "# create config for mmcv\n",
    "def create_config_from_file(\n",
    "        conf_path: str = 'configs/pspnet/pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py',\n",
    "        load_from: str = 'pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth'\n",
    "    ):\n",
    "    cfg = Config.fromfile(conf_path)\n",
    "\n",
    "    # Since we use only one GPU, BN is used instead of SyncBN\n",
    "    cfg.norm_cfg = dict(type='BN', requires_grad=True)\n",
    "    cfg.crop_size = (256, 256)\n",
    "    cfg.model.data_preprocessor.size = cfg.crop_size\n",
    "    cfg.model.backbone.norm_cfg = cfg.norm_cfg\n",
    "    cfg.model.decode_head.norm_cfg = cfg.norm_cfg\n",
    "    cfg.model.auxiliary_head.norm_cfg = cfg.norm_cfg\n",
    "    # modify num classes of the model in decode/auxiliary head\n",
    "    cfg.model.decode_head.num_classes = 2\n",
    "    cfg.model.auxiliary_head.num_classes = 2\n",
    "\n",
    "    # cfg.model.optimizer = dict(lr=0.005, momentum=0.9, type='SGD', weight_decay=0.0005)\n",
    "\n",
    "    # Modify dataset type and path\n",
    "    cfg.dataset_type = 'StanfordBackgroundDataset'\n",
    "    cfg.data_root = ''\n",
    "\n",
    "    cfg.train_dataloader.batch_size = 8\n",
    "\n",
    "    cfg.train_pipeline = [\n",
    "        dict(type='LoadImageFromFile'),\n",
    "        dict(type='LoadAnnotations'),\n",
    "        # dict(type='RandomResize', scale=(2048, 2048), ratio_range=(0.5, 2.0), keep_ratio=True),\n",
    "        # dict(type='RandomCrop', crop_size=cfg.crop_size),\n",
    "        # dict(type='RandomFlip', prob=0.5),\n",
    "        dict(type='PackSegInputs')\n",
    "    ]\n",
    "\n",
    "    cfg.test_pipeline = [\n",
    "        dict(type='LoadImageFromFile'),\n",
    "        # dict(type='Resize', scale=(2048, 2048), keep_ratio=True),\n",
    "        # add loading annotation after ``Resize`` because ground truth\n",
    "        # does not need to do resize data transform\n",
    "        dict(type='LoadAnnotations'),\n",
    "        dict(type='PackSegInputs')\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Load the pretrained weights\n",
    "    cfg.load_from = load_from\n",
    "\n",
    "    # Set up working dir to save files and logs.\n",
    "    cfg.work_dir = './work_dirs/tutorial'\n",
    "\n",
    "    train_cfg = dict(type='EpochBasedTrainLoop')\n",
    "\n",
    "    cfg.train_cfg.max_iters = 10000\n",
    "    cfg.train_cfg.val_interval = 1000\n",
    "    cfg.default_hooks.logger.interval = 100\n",
    "    cfg.default_hooks.checkpoint.interval = 1000\n",
    "\n",
    "    # Set seed to facilitate reproducing the result\n",
    "    cfg['randomness'] = dict(seed=0)\n",
    "\n",
    "    return cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parts of the U-Net model \"\"\"\n",
    "# Unet baseline realisation\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def use_checkpointing(self):\n",
    "        self.inc = torch.utils.checkpoint(self.inc)\n",
    "        self.down1 = torch.utils.checkpoint(self.down1)\n",
    "        self.down2 = torch.utils.checkpoint(self.down2)\n",
    "        self.down3 = torch.utils.checkpoint(self.down3)\n",
    "        self.down4 = torch.utils.checkpoint(self.down4)\n",
    "        self.up1 = torch.utils.checkpoint(self.up1)\n",
    "        self.up2 = torch.utils.checkpoint(self.up2)\n",
    "        self.up3 = torch.utils.checkpoint(self.up3)\n",
    "        self.up4 = torch.utils.checkpoint(self.up4)\n",
    "        self.outc = torch.utils.checkpoint(self.outc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = create_config_from_file(CCNET_CONF_PATH, load_from=CCNET_CHECKPOINT_PATH)\n",
    "ccnet_model = init_model(cfg, CCNET_CHECKPOINT_PATH, 'cuda:0')\n",
    "\n",
    "cfg = create_config_from_file(MMCV_CONF_PATH, load_from=MMCV_CHECKPOINT_PATH)\n",
    "pspnet_model = init_model(cfg, MMCV_CHECKPOINT_PATH, 'cuda:0')\n",
    "\n",
    "unet_model = UNet(3, 1, False).to(device)\n",
    "unet_model.load_state_dict(torch.load(UNET_CHECKPOINT_PATH))\n",
    "unet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess functions and predictions for using in blending\n",
    "def unet_preprocess(pil_img, is_mask):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((256, 256))\n",
    "    ])\n",
    "    img = np.asarray(pil_img)\n",
    "\n",
    "    if transform:\n",
    "        img = transform(img)\n",
    "\n",
    "    if is_mask:\n",
    "        img *= 255\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def predict_img(net, img, device):\n",
    "    net.eval()\n",
    "    img = np.array(img)\n",
    "    img = unet_preprocess(img, is_mask=False)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device=device, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        output = net(img).cpu()\n",
    "        mask = torch.sigmoid(output)\n",
    "    return mask.squeeze().numpy()\n",
    "\n",
    "\n",
    "def read_image_from_mmcv(img_path):\n",
    "    return mmcv.imread(img_path)\n",
    "\n",
    "\n",
    "def read_image_to_numpy(img_path):\n",
    "    return np.array(Image.open(img_path))\n",
    "\n",
    "def mmcv_predict(img_path, get_buildings=True):\n",
    "    # Init the model from the config and the checkpoint\n",
    "    img = mmcv.imread(img_path)\n",
    "\n",
    "    result = torch.softmax(inference_model(pspnet_model, img).seg_logits.data, dim=0)\n",
    "    if get_buildings:\n",
    "        result = result[0]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def ccnet_predict(img_path, get_buildings=True):\n",
    "    # Init the model from the config and the checkpoint\n",
    "    img = mmcv.imread(img_path)\n",
    "\n",
    "    result = torch.softmax(inference_model(ccnet_model, img).seg_logits.data, dim=0)\n",
    "    if get_buildings:\n",
    "        result = result[0]\n",
    "\n",
    "    return result\n",
    "\n",
    "def unet_predict(img, device=device):\n",
    "    return predict_img(unet_model, img, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Blending:\n",
    "    def __init__(self, predict_functions, weights_list, read_picture_functions):\n",
    "        self.predict_functions = predict_functions\n",
    "        self.weights_list = weights_list\n",
    "        assert abs(sum(self.weights_list) - 1) <= 1e-8\n",
    "        self.read_picture_functions = read_picture_functions\n",
    "\n",
    "    def predict(self, img):\n",
    "        if isinstance(img, str):\n",
    "            pil_img = np.array(Image.open(img))\n",
    "        elif isinstance(img, np.ndarray):\n",
    "            pil_img = img\n",
    "        else:\n",
    "            raise ValueError(f\"Type of img: {type(img)}. The type is not known\")\n",
    "\n",
    "        res_mask = np.zeros((pil_img.shape[0], pil_img.shape[1]))\n",
    "        for i, (pred_func, read_pic_func) in enumerate(zip(self.predict_functions, self.read_picture_functions)):\n",
    "            img = read_pic_func(img)\n",
    "            pred_mask = pred_func(img)\n",
    "            pred_mask = cv2.resize(\n",
    "                pred_mask.cpu().detach().numpy(), \n",
    "                dsize=(pil_img.shape[1], pil_img.shape[0]), \n",
    "                interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            pred_mask = 1 - pred_mask\n",
    "            res_mask += (self.weights_list[i] * pred_mask)\n",
    "\n",
    "        return res_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission for blending\n",
    "\n",
    "PATCH_SIZE = 256\n",
    "TEST_DATA_PATH = '/kaggle/input/leaders-of-digital-segmentation-test/images/*'\n",
    "\n",
    "\n",
    "def predict_full(blend_obj, path):\n",
    "    img = cv2.imread(path)\n",
    "    mask = np.zeros((img.shape[0], img.shape[1]))\n",
    "    count = np.zeros((img.shape[0], img.shape[1]))\n",
    "    for i in tqdm(range(0 * 128, (img.shape[0] + PATCH_SIZE - 1) // PATCH_SIZE * 256, 128)):\n",
    "        for j in range(0 * 128, (img.shape[1] + PATCH_SIZE - 1) // PATCH_SIZE * 256, 128):\n",
    "            patch_img = img[i:i + PATCH_SIZE, j:j + PATCH_SIZE]\n",
    "            if np.prod(patch_img.shape) == 0:\n",
    "                continue\n",
    "            predicted = blend_obj.predict(patch_img)\n",
    "            mask[i:i + PATCH_SIZE, j:j + PATCH_SIZE] += predicted\n",
    "            count[i:i + PATCH_SIZE, j:j + PATCH_SIZE] += np.ones(predicted.shape)\n",
    "\n",
    "    return mask, count\n",
    "\n",
    "\n",
    "for fn in sorted(glob(TEST_DATA_PATH)):\n",
    "    mask, count = predict_full(fn)\n",
    "    cv2.imwrite(fn.split('/')[-1].replace('image', 'mask'), mask / count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
